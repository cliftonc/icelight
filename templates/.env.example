# cdpflare Environment Configuration
# Copy this file to .env and update values

# ============================================
# Ingestion Worker Configuration
# ============================================

# Enable authentication for ingestion endpoints
# Set to "true" to require API token for ingestion
AUTH_ENABLED=false

# API token for ingestion authentication (if AUTH_ENABLED=true)
# Generate a secure random token: openssl rand -hex 32
AUTH_TOKEN=your-secret-ingestion-token

# Allowed CORS origins (comma-separated)
# Use "*" to allow all origins (not recommended for production)
ALLOWED_ORIGINS=*

# ============================================
# Query Worker Configuration
# ============================================

# Cloudflare Account ID
# Find at: https://dash.cloudflare.com → select account → Overview → Account ID
CF_ACCOUNT_ID=your-cloudflare-account-id

# Cloudflare API Token
# Create at: https://dash.cloudflare.com/profile/api-tokens
# Required permissions: R2 Data read/write
CF_API_TOKEN=your-cloudflare-api-token

# R2 SQL Warehouse name
# Created automatically when you enable Data Catalog on an R2 bucket
WAREHOUSE_NAME=your-warehouse-name

# Optional: API token to require for query API access
# If not set, query API is open (CORS still applies)
# API_TOKEN=your-query-api-token

# ============================================
# Pipeline Configuration (for setup script)
# ============================================

# R2 bucket name for storing event data
BUCKET_NAME=cdpflare-data

# Pipeline resource names
PIPELINE_NAME=cdpflare-events-pipeline
STREAM_NAME=cdpflare-events-stream
SINK_NAME=cdpflare-events-sink

# Iceberg table location
NAMESPACE=analytics
TABLE_NAME=events

# Compression for Parquet files (zstd, snappy, gzip, none)
COMPRESSION=zstd

# Roll interval in seconds (how often to write new Parquet files)
ROLL_INTERVAL=60
